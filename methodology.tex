\documentclass[thesis.tex]{subfiles}

\begin{document}
\chapter{The algorithm ``Fuzzy context-based search''}
{\parindent0pt
In this chapter we introduce the algorithm ``Fuzzy context-based search'' ("fuzzy search" or "fuzzy" for short) as a solution to the problem of aligning text strings against graph based reference genomes. In order to do this we will first present formal definitions of the elements and structures involved as well as the problem itself. The following description of the algorithm will be a conceptual overview where the motivation behind the steps taken are also described. A more detailed introduction to a precise implementation of the algorithm will follow in the succeeding chapter, in which space and time complexity will also be discussed. Due to the abstract nature of this chapter the reader is advised to use the coming chapter as a reference whenever needed. The two have corresponding sections, the latter contain exact details and concrete examples. There can also be value in looking up the visualizations of actual runtime examples shown in Chapter \ref{sec:validation}.\\
\par\noindent
In order to avoid ambiguity when dealing with already existing concepts, the terms which are defined are given problem-specific names. For several of the terms there also follows a shorthand notation behind the original name in the definition title. Whenever these shorthand names are used in the subsequent explanatory sections we refer exclusively to the definitions done in this thesis.
\section{The graphs}
The graphs used as reference genome graphs will be built iteratively by starting out with an empty graph and sequentially merging in input sequences aligned against the existing structure. How the sequences are merged, and thus what the graphs look like, are decided entirely through the alignment procedure, which in part relies on the scoring schema. This first section is dedicated to precisely defining the involved graphs through definitions of their constituents. 
\clearpage
\begin{defn}[Graph based reference genome (Graph)]
  A pair $G=\{V,E\}$ where $V$ is a set of vertices and $E$ is a set of edges. $|G|$ denotes the number of vertices in $G$.
\end{defn}
The involved graphs will be sequence graphs\footnote{Discussed in section \ref{sec:sequence_graphs}} where every vertex correspond to a single nucleotide from a one or more input sequences used in building the graph. Whether the vertex originates from a single or several sequences is based on whether any new bases has been mapped, and consequently merged, into the vertex. In addition to the nucleotide the vertices will contain an index which is unique to the graph. 
\begin{defn}[Graph genome vertex (Vertex)]
  A pair $v=\{b, i\}$ where $b \in \{A, C, T, G\}$ and $i$ is a unique index. The vertice at index $i$ is denoted $v_i$. The notation $b(v_i)$ is used to reference the first element in the pair (the nucleotide).
\end{defn}
Every graph $G$ will have two special vertices $s_G=\{s, 0\}$ and $t_G=\{e, -1\}$ which represents unique start and end vertices.\\
\par\noindent
The edges in the graph model the relationships between the vertices and thus the relationships between the elements of the input sequences. Every edge has its origin from a consecutive pair of nucleotides in one or more input sequences.
\begin{defn}[Graph genome edge (Edge)]
  An ordered pair $e=\{i_s, i_e\}$ where both elements are indexes for vertices. 
\end{defn}
There exists no information storing the origin of an edge, or whether an edge originates from one or more input sequences, and all edges are thus seen as equally probable when aligning a sequence. A sequence of vertices where there exists an edge for every pair of consecutive vertices is called a \textit{path}. The introduction of paths is a a way of capturing the combination of several individual characters into text strings in the domain of our graphs.
\begin{defn}[Graph genome path (Path)]
  A list $P$ of indexes such that for all consecutive pairs $(p_x, p_{x+1}) \in P$, where $p_n$ denotes the n-th element of the list, there exists an edge $e=\{p_x, p_{x+1}\}$. The notation $p_{-1}$ denotes the last element in the list. The length of $P$, $l(P)$, is equal to the number of indexes in the list. We define the distance $d(P)$ between $p_0$ and $p_{-1}$ as $l(P) - 2$.
\end{defn}
\begin{corollary}[Distance between neighbours]
  Every edge $e$ is also a path $P$ with $d(P)=0$.
\end{corollary}
Paths spanning the entire length of a graph $G$, from $s_G$ to $t_G$, are named full paths. Every input sequence used to build the graph has a corresponding full path.
\begin{defn}[Full path]
  A path $P$ through a graph $G$ where $p_0=0$ and $p_{-1}=-1$
\end{defn}
There is no correspondence the other way, meaning there can exist full paths which does not originate from a single input sequence. An example of this can be seen in figure \ref{fig:example_reference} where a reference graph made from three sequences has 9 valid full paths. When aligning regular text strings against each other the introduction of gaps is a key element. We translate this concept to the graph domain through the introduction of \textit{incomplete paths}.
\begin{defn}[Incomplete path]
  A list $P*$ of indexes such that for all consecutive pairs $(p*_x, p*_{x+1}) \in P*$ there exists a path $P$ such that $p_0=p*_x$ and $p_{-1}=p*_{x+1}$.
\end{defn}
Conceptually incomplete paths can be seen as regular paths where some of the vertices are removed to reflect gaps. An example of an incomplete path seen in figure \ref{fig:example_reference} is $[1, 2, 4, 5]$. We can score an incomplete path by looking solely at the gaps present and avoiding mapping scores for the nucleotides contained in the vertices to produce a \textit{path score}. 
\begin{defn}[Path score]
  The total score of all gaps present in an incomplete path $P*$ according to a scoring schema
\end{defn}
In an incomplete path there exists two possible relationships between consecutive elements: Either they are neighbours and there exists an edge between them, or they are not neighbours and are at the beginning and end of a path. Because the edges have distance $0$ and are thus not penalized, the path score of an incomplete path can be found by summarizing gap penalties for gaps between every pair of consecutive vertices:
\begin{equation}
  \label{eq:path_score}
  pathScore(P*)=\Sigma^{|P*|-2}_{i=0} gapPenalty(distance(p*_i, p*_{i+1}))
\end{equation}
where $distance(x, y)$ denotes the distance of the shortest path $P$ where $P_0=x$ and $P_{-1}=y$. If we continue with the example incomplete path $[1, 2, 4, 5]$ from the figure, we can see one pair on consecutive vertices which are not neighbours: $(2, 4)$. We can see that the only path between them, the path $[2, 3, 4]$, has a distance of $1$.
\begin{figure}[b]
  \begin{mdframed}
    \begin{center}
      \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=1.4cm,scale=0.5]
        \node[state,align=center] (q0) {$s$\\ \scriptsize{0}};
        \node[state,align=center] [right of=q0] (q1) {$A$\\ \scriptsize{1}};
        \node[state,align=center] [above right=0.6cm and 0.63cm of q1] (q2) {$T$\\ \scriptsize{2}};
        \node[state,align=center] [right of=q1] (q3) {$G$\\ \scriptsize{6}};
        \node[state,align=center] [below right=0.6cm and 0.63cm of q1] (q4) {$C$\\ \scriptsize{8}};
        \node[state,align=center] [right of=q3] (q5) {$A$\\ \scriptsize{3}};
        \node[state,align=center] [above right of=q5] (q6) {$T$\\ \scriptsize{4}};
        \node[state,align=center] [below right of=q5] (q7) {$G$\\ \scriptsize{7}};
        \node[state,align=center] [below right of=q6] (q8) {$A$\\ \scriptsize{5}};
        \node[state,align=center] [right of=q8] (q9) {$e$\\ \scriptsize{-1}};

        \path
        (q0) edge node {} (q1)
        (q1) edge node {} (q2)
        edge node {} (q3)
        edge node {} (q4)
        (q2) edge node {} (q5)
        (q3) edge node {} (q5)
        (q4) edge node {} (q5)
        (q5) edge node {} (q6)
        edge node {} (q7)
        edge node {} (q8)
        (q6) edge node {} (q8)
        (q7) edge node {} (q8)
        (q8) edge node {} (q9);
      \end{tikzpicture}
    \end{center}
  \end{mdframed}
  \caption{An example reference graph $G$ made from the three sequences ``ATATA'', ``AGAGA'' and ``ACAA''}
  \label{fig:example_reference}
\end{figure}
}
\clearpage
\section{The alignment problem}
{\parindent0pt
Defining the graphs mean we have a formal notion of half of the input data for the alignment problem as discussed in section \ref{sec:mapping}. We move on to defining the other half: The \textit{input sequences}.
\begin{defn}[Input sequence]
  A string $s$ over the alphabet $\{A, C, T, G\}$. The length of the string is given by $|s|$. The individual character on position $0<=x<|s|$ is referenced by $s_x$. A substring spanning the characters from $x$ to $y$ is denoted $s_{x:y}$
\end{defn}
Both in defining the graph vertices and the input strings we put a limitation on the legal characters by defining their alphabets. This is done to stick with the concept of genetic information. The approach is however general enough to handle arbitrarily large and complex alphabets.\\
\par\noindent
Once we have a clear definition of a graph $G$ and an input sequence $s$ we can specify what an \textit{alignment} between the two should look like, representing a model of the relationship between them. In order to achieve this goal, the alignments should provide relations between the smallest constituents of the two input structures, the vertices of the graph and the characters of the string, in a way such that the interal structures of the two are reflected against eachother. We can model an alignment as a special variant of an incomplete path, which allows for \textit{unmapped elements}. These elements are recognized as elements of $s$ which is mapped to $0$, the index of the start-vertice, and thus always an invalid mapping. The remaining elements of $s$ is mapped to indexes of valid vertices of $G$ which form an incomplete path $P*$. Moving forward through the individual positions $s_x$ which are mapped corresponds to traversing $P*$.
\begin{defn}[Alignment]
  Given a graph $G$ and a string $s$, an alignment $A$ is an ordered list of length $|s|$ such that every element $a_x \in A$ is either $0$ or the index for a valid vertice of $G$ such that for every consecutive pair of valid indexes $a_n, a_m$ there exists a path $P$ where $p_0=a_n$ and $p_{-1}=a_m$. A $0$ represents an unmapped character in $s$.
\end{defn}
When we have defined the alignments we can start scoring them. The scoring happens according to a scoring schema and should be the sum of three different scores:
\begin{enumerate}
  \item The mapping scores of the mapped elements
  \item The gap penalties for gaps in the graph, represented by the path score of the incomplete path P*
  \item The gap penalties for gaps in the string, represented by unmapped positions
\end{enumerate}
The first two can be looked up through the standard functionality provided by the scoring schema and equation \ref{eq:path_score}. The last can be found by summing up the gap penalties for all the gaps in the input sequence. A gap in the input sequence can be identified by a continuous subsequence $A*_{x:y} \in A$ spanning the indexes $x$ to $y-1$ where every element is unmapped. An important aspect here is that every unmapped element should only be considered part of exactly one gap. We cover this aspect by only considering \textit{maximal unmapped subsequences}
\begin{defn}[Maximal unmapped subsequence]
  A subsequence $A*_{x:y} \in A$, such that every $a*=0$ for every $a* \in A*$ and $x$ is either $0$ or $a_{x-1} \neq 0$ and $y$ is either $|s|-1$ or $a_{y+1} \neq 0$.
\end{defn}
The gap penalties for gaps in the string is then defined as 
\begin{equation}
  stringGap(A)=\Sigma_{A* \in A_{MUS}} gapPenalty(|A*|)
\end{equation}
where $A_{MUS}$ is the set of maximal unmapped subsequences in $A$. Once we have clear definitions of the three elements we can define the score itself:
\begin{defn}[Alignment score]
  Given a sequence $s$, a graph $G$ and an alignment $A$, the score produced by combining mapping scores for the pairs $\{b(v_{a_x}), s_x\}$ for $0<=x<|s|$ with the path score for the incomplete path provided by consecutive mapped indexes of $A$ and the gap penalties for the string gaps in $A$. We reference this score by $alignmentScore(A)$.
\end{defn}
We can then easily define our graph based adaptation\footnote{The regular alignment problem for strings was presented in section \ref{sec:alignment}} of the alignment problem:
\begin{defn}[The optimal alignment score problem]
  For any pair $\{G, s\}$, where $G$ is a graph and $s$ is an input sequence, find one of the alignments $A$ which produces the highest possible alignment score.
\end{defn}
Notice that the definition only calls for finding one of the alignments which produce the highest possible score. This is done in order to simplify the conceptual explanations of the algorithm. Implementation-wise this can trivially be changed to finding all optimal alignments. The necessary adjustments is discussed as a part of the succeding chapter in section \ref{sec:all_optimal_alignments}.
\begin{figure}[H]
  \begin{mdframed}
    \begin{center}
  \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=1.4cm]
    \node[state] (q0) {$s$};
    \node[state] [right of=q0] (q1) {$A$};
    \node[state] [right of=q1] (q2) {$T$};
    \node[state] [right of=q2] (q3) {$T$};
    \node[state] [right of=q3] (q4) {$G$};
    \node[state] [right of=q4] (q5) {$T$};
    \node[state] [right of=q5] (q6) {$C$};
    \node[state] [right of=q6] (q7) {$e$};
    \node[state,draw=none] [above of=q3] (q8) {$G$};
    \node[state,draw=none] [left of=q8] (q9) {$T$};
    \node[state,draw=none] [left of=q9] (q10) {$A$};
    \node[state,draw=none] [above of=q4] (q11) {$A$};
    \node[state,draw=none] [above of=q5] (q12) {$T$};

    \path (q0) edge node {} (q1)
    (q1) edge node {} (q2)
    (q2) edge node {} (q3)
    (q3) edge node {} (q4)
    (q4) edge node {} (q5)
    (q5) edge node {} (q6)
    (q6) edge node {} (q7)
    (q8) edge[color=red] node {} (q4)
    (q9) edge[color=red] node {} (q2)
    (q10) edge[color=red] node {} (q1)
    (q12) edge[color=red] node {} (q5);
  \end{tikzpicture}
    \end{center}
  \end{mdframed}
  \caption{A visualization of an example alignment of the string "ATGAA" against a reference genome made from the string "ATTGTC". The actual alignment is visualized through red arrows pointing from the characters of the string to the vertex they are mapped to. We see a gap in the incomplete path between the second and third vertex and that the fourth element of the string is unmapped}
\end{figure}
\clearpage\noindent
Additionally we have defined a bounded version of the problem, which we call \textit{The bounded optimal alignment score problem}. This second version also considers a score threshold value $T$ and deems a string $s$ \textit{unalignable} if the optimal alignment produces a score lower than T.
\begin{defn}[The bounded optimal alignment score problem]
  \label{def:bounded_alignment_problem}
  Given a triplet $\{G, s, T\}$ where $G$ and $s$ are as before and $T$ is a numeric value, find the alignment $A$ which produces the highest alignment score, if and only if the alignment score for $A$ is higher than $T$. If no such alignment exists, $s$ should be classified as unalignable.
\end{defn}
Defining a bounded adaptation of the problem is obviously done in order to reduce the computational complexity, but it also present a powerful notion of control to the model: We can choose the degree of similarity required for substructures to be considered equal. This simplifies the concept of equality into a classification problem where the border between the two classes can be easily manipulated through the threshold variable.\\
\par\noindent
At this point we want to point out a distinction which does not become clear through the definition of the problem. The main goal of the algorithm is aligning short reads against a large reference. The problem definition does not in any way concern itself with the sequence length. We therefore assume the approach is used as a basis for building the graphs, by aligning longer sequences and merging based on this alignment. The length of $s$ does in no way interfere with the validity of the approach, but can be of interest to the user when considering the complexity analysis we do underway.
}
\clearpage
\section{``Fuzzy context-based search''}
Having properly defined the problem, we now present our algorithm as a proposed solution. The algorithm consists of two distinct subproblems which are solved in consecutive steps:
\begin{enumerate}
  \item Create a candidate graph $G'$ for an input triplet $\{G, s, T\}$
  \item Search $G'$ for an optimal alignment
\end{enumerate}
Both the motivation behind each step and the conceptual approach for solving the subproblem will be explained in its corresponding subsection. In addition to the three involved components set as input parameters, the algorithm assumes a predefind scoring schema.\\
\par\noindent
In presenting the algorithm we introduce a new variable $\lambda$. $\lambda$ represents the \textit{error margin} allowed in an alignment and is computed by taking the difference between the highest possible alignment score for $s$ and the scoring threshold $T$. In order to compute the highest possible alignment score for any string, we put a bound on our scoring schemas by introducing \textit{consistent scoring schemas}. A scoring schema is consistent if the highest possible alignment score for any string $s$ is achieved by aligning the string against itself. This presents us with an easy computation for finding the score we need. Introducing $\lambda$ gives us the opportunity to do strict pruning throughout the entire alignment process: Any alignment which contains a single element, be it a gap or a sequence of mappings, which is penalized more than $\lambda$ compared to the corresponding element in an optimal alignment can never have a total alignment score higher than $T$ (This should hopefully become apparent throughout this chapter, but a more compact proof can be found in Appendix \ref{sec:proof}).
\subsection{Constructing the candidate graph}
The motivation behind building an entirely new graph is the realization that whenever reads are mapped against a reference genome the read is typically vastly shorter than the reference. We can therefore do a \textit{horizontal pruning} where we determine which sections along the horizontal axis of the graphs are interesting for the alignment. The same argument can be made for extremely complex graphs, where only a small number of the branches are relevant, in an operation we have called a \textit{vertical pruning}. The result should be a new graph $G'$ with a vertice set $V'$ and an edge set $E'$.\\
\par\noindent
We first let $V'$ be a subset of the original vertex set $V$. In order to guarantee optimality we put a restriction on $V'$:
\begin{enumerate}
  \item Every $v_x \in V$ should be in $V'$ if there exists an optimal alignment $A$ with an alignment score $\varphi_A >= T$ which contains the index $x$
\end{enumerate}
Through the definition of the alignments we know they are ordered and that the indexed elements refer to the vertices which map to a specific position in the string. We can use this knowledge to more specifically define $V'$ as an ordered set of sets $V'_x$ where every indexed set is related to the corresponding position in the alignment:
\begin{enumerate}
  \item Every $v_x \in V$ should be in $V'_y$ if there exists an optimal alignment $A$ with a higher score than $T$ where $a_y=x$
\end{enumerate}
This is a restriction which is strictly enforced throughout the algorithm, to continue ensuring the optimal solution exists as a possibility. We formulate a second restriction, to reduce the number of vertices we identify as not interesting for the final alignment:
\begin{enumerate}
  \setcounter{enumi}{1}
  \item Every $v_x \in V$ which is not referenced by $a_i$ in any optimal alignment should not be in $V'_i$
\end{enumerate}
If we manage to create $V'$ from these two restrictions we can guarantee a vertice set where every element of every optimal alignment is still present and all excesses vertices has been dropped. However, finding these vertices requires knowledge of every alignment $A$ of every string $s$ for every threshold $T$, a number of possibilities which quickly become infeasible. In order to make the operation more tractable we identify the second restriction as being related solely to the computational complexity, which means it does not have to be strictly enforced. We can thus relax it without interfering with the principle of optimality:
\begin{enumerate}
  \setcounter{enumi}{1}
  \item Every vertice $v_x \in V$ should be in $V'_i$ for every $0<=i<|s|$.
\end{enumerate}
This is a complete relaxation and puts every vertice $v \in V$ in every subset of $V'$. The resulting parenting candidate vertice set $V'$ is a set far greater than $V$ which is obviously suboptimal for the following search. These two cases represent the two extremes on the scale of how strictly we enforce the second restriction and they both represent problems: Either the search is too complex or the result is too inaccurate. We can let the second restriction be an informal description of a search for an optimal middle ground between the two:
\begin{enumerate}
  \setcounter{enumi}{1}
  \item Every subset $V'_x$ should be \textit{as small as possible, without the search growing too complex}
\end{enumerate}
The rest of this section will describe our method for approximating this middle ground\\
\par\noindent
We let a vertice $v$ be a \textit{candidate vertex} for index $i$ if it is a part of the \textit{candidate set} $V'_i$. In order to find candidate vertices we apply \textit{fuzzyness} to the context-based mapping schema proposed by Paten et al\footnote{Context-based mapping is explained in detail in section \ref{sec:mapping}}. We say a vertex is a candidate vertex for an index if it has a context which is similar enough to the context of the corresponding position $s_i$ in $s$. The vagueness of ``similar enough'' is controlled through the fuzzyness, which again is controlled through the error margin parameter $\lambda$. The contexts of the vertice represents the paths passing through it, and because we know that if a context is penalized more than $\lambda$ compared to the maximal possible score the context can never be a part of a longer incomplete path with a total score higher than $T$. Thus, more formally, for every index $0<=i<|s|$ we put $v_x$ in $V'_i$ if and only if the context set $c(v_x)$ of $v_x$ contains a context which can be aligned against $c(s_i)$ with a score higher than $T_c$. When we refer to the context set $c(s_n)$ for elements of the string, we simply mean the only linear context possible, a substring of $s$ surrounding position $n$. $T_c$ is a \textit{context threshold score} and is computed by taking the max possible score for a context in $s$ and subtract $\lambda$.\\
\par\noindent
After deciding which vertices make up $G'$ we need to decide how we combine them, through the edge set $E'$. Because the subsets of candidate vertices follow a natural ordering there is already defined a direction in the graph. Every vertex of every candidate set $V'_i$ should have an incoming edge from every vertex in the preceding candidate set $V'_{i-1}$ to account for this direction. Because we allow gaps in our alignments we have to extend the number of steps a vertex looks back for possible paths: Every vertex in every candidate set $V'_i$ should have an incoming edge from every vertex in \textit{every} preceding candidate set $V'_{j}$, where $0<=j<i$. These edges represent the relationships between the elements of the string. We also want to represent the relationships between the vertices in the graph they originate from. This is done through the introduction of \textit{weighted edges}:
\begin{defn}[Graph genome weighted edge (Weighted edge)]
  A triplet $e'=\{i_s, i_e, w\}$ where the two first elements are indexes for vertices in $V'$ and the latter is an integer
\end{defn}
\noindent
We let the weight $w$ denote the distance $d(P)$ of the shortest path $P$ where $P_0=i_s$ and $P_{-1}=i_e$. These weights can be found through a regular graph search in $G$, if no distance is found we let the value be $\inf$. At this point we have a complete graph $G'$.
\begin{corollary}[Weighted edges for neighbours]
  For every edge $e=\{i_s, i_e\} \in E$ where $v_{i_s} \in V'_x, v_{i_e} \in V'_y$ and $x<y$ there exists a weighted edge $e'=\{i_s, i_e, 0\} \in E'$
\end{corollary}
\noindent
The resulting graph is still very complex . Every vertice is connected to every preceding vertice, and in order to find the weights of these edges we need to do graph searches for every possible pair of vertices. However, we still know we are not interested in incomplete paths which have a lower score than $T$ and we thus can limit the edges to only the ones that are passable without being penalized more than $\lambda$. This creates an upper bound both on how far back in the candidate sets a vertice looks for neighbours and, more importantly, the complexity of the individual graph searches done in $G$.
\subsection{Searching the newly formed graph}
We have built $G'$ in a specific way to guarantee the optimal alignments still exist, which means the next step is finding them. Searching for an alignment means combining vertices, representing bases, into a path representing a string. This linear sequence can be aligned against the input sequence with regular string alignment tools and the scores are therefore easily verifiable.\\
\par\noindent
In order to continue securing optimal results the algorithm does the search using exhaustive dynamic programming. The search algorithm is conceptually very similar to PO-MSA\footnote{The DP algorithm developed in \cite{multiple_sequence_alignment_using_partial_order_graphs} presented in \ref{sec:dp_on_graphs}}, except the roles are switched around: Instead of searching through the reference graph with an input string we are searching through the indices of the string with the canditate vertices from the reference graph as our input. When we dynamically compute scores we are still doing the same thing as a regular PO-MSA, letting a candidate vertice $v_x$ in a candidate set $V'_i$ be an intersection at position $x, i$ in a two-dimensional space where the dimensions represent the string and the path. We let an individual score identified by $x, i$ be the highest possible score for aligning the substring $s_{0:i}$against a path ending in $v_x$. In this way we can find the highest possible score for the entire alignment in the highest scoring node in the last candidate set.\\
\par\noindent
The base case of the dynamic programming are the candidate vertices in the first candidate set, $v_x \in V'_0$. We initialize these scores to $mappingScore(b(v_x), s_0)$, which is equivalent to aligning them against the substring containing exactly the first character of the string. During the following bottom-up procedure we will be faced with another set of base cases: Vertices which have no incoming edges. If the vertices are reachable by gapping over the preceding indexes of the string without the gap penalty exceeding $\lambda$ we initialize them to their mapping score combined with the gap penalty. In all other cases we set the score to an arbitrary low value, for instance $-2 * \lambda$, which yields any alignment starting with the vertex a score lower than $T$. This means they can not be considered candidates for the optimal alignment.\\
\par\noindent
The recurrence relation of the dynamic programming algorithm is concerned with setting the score for any vertice/index pair which is not a base case. The score for these candidate vertices $v_x \in V'_i$ are set by looking at all incoming edges, find the one yielding the highest score and add $mappingScore(b(v_x), s_y)$. The score for an edge is found by taking the score in the vertice $v_y \in V'_j$,represented by the start-index $i_s$ in the edge, and adding the gap penalties corresponding to traversing the edge. There are two gap penalties related to the edge: one penalty for the distance represented by the weight $w$ and one penalty for jumping from index $i$ to index $j$. However, all edges traversed in the final alignment will only be penalized for one of them. We know this because whenever there exists an alternative with only one gap, this will be prioritized due to a lower gap penalty. Whenever there does not exist such an alternative, this means the candidate vertex which "should" have existed is not a member of any contexts scoring high enough, meaning the path is never interesting when creating an alignment with a score higher than $T$.\\
\par\noindent
When the scores have been computed for every candidate vertice we can start looking for the highest score, which represents the alignment score for the optimal alignments. We will find this score as a score for one of the vertices in the candidate set corresponding to the last index of the string. At this point we just have to backtrack the procedure which lead to the score to find the actual alignment, which is guaranteed to be one of the optimal alignments. If we find no score higher than the threshold $T$ we can simply deem the string as unalignable.
\section{Heuristical applications}
\label{sec:heuristical_conceptual}
In this chapter we have presented a strict non-heuristical algorithm which is guaranteed to give the correct results whenever it is fed the correct parameters. This is a concept which provides formally steady ground when discussing the tractability of the conceptual solution to the problem. When nothing else is specifically stated, the remainder of the thesis will be concerned with the approach presented up to this point, in all its formally provable glory. However, we now want to take a brief detour to examine some of the heuristical possibilities which emerges. In the approach, if we are not able to find a good enough score for a vertex in the last candidate set we simply classify the string as unalignable. This might seem a bit rash, as we have already completed a strenuous search for similarities between the two structures. As previously mentioned this is a design choice taken to land at an easily perceivable, formally strict, proof-of-concept algorithm. If one were to imagine the strict optimality restriction were dropped, it is fair to assume that there exists a set of "good" solutions in the combination of the entities we have found in the search. We can see one such example in figure \ref{fig:heuristic_encouragement}: Because there are three mismatches the regular approach would need an error-margin $\lambda\geq3$ to find the alignment\footnote{Assuming regular edit distance scoring}. If we set the context length $|c|\leq2$ there are no vertices which has more then a single mismatch in their contexts. Because of this we are able find all the candidate vertices, and the path through them, for the optimal alignment with $\lambda=1$. The algorithm will however drop it as a solution because of the alignment score dropping below the threshold. We propose a heuristical approach would present the found, and in this case optimal, solution. The technical details on how this can be implemented is described in section \ref{sec:implementation_heuristic} of the next chapter.\\
\par\noindent
There is a specific reason for devising an algorithm which sometimes discards the optimal solution. In figure \ref{fig:heuristic_encouragement_b} we can see an alignment with two mismatches. If we let set $|c|=2$ and $\lambda=1$ there will be candidate vertices which are dropped due to pruning in the initial context search. If we imagine the two reference graphs in the figure are actually two different paths in the same graph, this will result in the heuristical algorithm we propose discovering the path with three mismatches and not the one with two, thus ending in a non-optimal alignment. If we utilize different error margins for the first and the second step of the algorithm, it is no longer given that the optimal path through the candidate graph is equivalent to the best path through the original graph. For the approach proposed up to this section we have shown guaranteed optimality by setting some constraints. This is the reason, when nothing else is specified, we will be concerned with the non-heuristical algorithm for the remainder of the thesis.
\begin{figure}[!hb]
    \begin{subfigure}[t]{\textwidth}
  \begin{mdframed}
  \begin{center}
  \begin{tikzpicture}[->,auto,node distance=1.2cm]
    \node[state,scale=0.67] (q0) {$s$};
    \node[state,scale=0.67] [right of=q0] (q1) {$A$};
    \node[state,scale=0.67] [right of=q1] (q2) {$A$};
    \node[state,scale=0.67] [right of=q2] (q3) {$A$};
    \node[state,scale=0.67] [right of=q3] (q4) {$A$};
    \node[state,scale=0.67] [right of=q4] (q5) {$A$};
    \node[state,scale=0.67] [right of=q5] (q6) {$A$};
    \node[state,scale=0.67] [right of=q6] (q7) {$A$};
    \node[state,scale=0.67] [right of=q7] (q8) {$A$};
    \node[state,scale=0.67] [right of=q8] (q9) {$A$};
    \node[state,scale=0.67] [right of=q9] (q10) {$A$};
    \node[state,scale=0.67] [right of=q10] (q11) {$A$};
    \node[state,scale=0.67] [right of=q11] (q12) {$A$};
    \node[state,scale=0.67] [right of=q12] (q13) {$A$};
    \node[state,scale=0.67] [right of=q13] (q14) {$e$};
    \node[state,scale=0.67,draw=none] [above of=q1] (q15) {$T$};
    \node[state,scale=0.67,draw=none] [right of=q15] (q16) {$A$};
    \node[state,scale=0.67,draw=none] [right of=q16] (q17) {$A$};
    \node[state,scale=0.67,draw=none] [right of=q17] (q18) {$A$};
    \node[state,scale=0.67,draw=none] [right of=q18] (q19) {$A$};
    \node[state,scale=0.67,draw=none] [right of=q19] (q20) {$A$};
    \node[state,scale=0.67,draw=none] [right of=q20] (q21) {$T$};
    \node[state,scale=0.67,draw=none] [right of=q21] (q22) {$A$};
    \node[state,scale=0.67,draw=none] [right of=q22] (q23) {$A$};
    \node[state,scale=0.67,draw=none] [right of=q23] (q24) {$A$};
    \node[state,scale=0.67,draw=none] [right of=q24] (q25) {$A$};
    \node[state,scale=0.67,draw=none] [right of=q25] (q26) {$A$};
    \node[state,scale=0.67,draw=none] [right of=q26] (q27) {$T$};

    \path (q0) edge node {} (q1)
    (q1) edge node {} (q2)
    (q2) edge node {} (q3)
    (q3) edge node {} (q4)
    (q4) edge node {} (q5)
    (q5) edge node {} (q6)
    (q6) edge node {} (q7)
    (q7) edge node {} (q8)
    (q8) edge node {} (q9)
    (q9) edge node {} (q10)
    (q10) edge node {} (q11)
    (q11) edge node {} (q12)
    (q12) edge node {} (q13)
    (q13) edge node {} (q14);
  \end{tikzpicture}
  \end{center}
  \end{mdframed}
  \caption{A sequence with three apparent SNPs aligned against a reference graph}
  \label{fig:heuristic_encouragement}
  \end{subfigure}
  \begin{subfigure}[b]{\textwidth}
  \begin{mdframed}
  \begin{center}
  \begin{tikzpicture}[->,auto,node distance=1.2cm]
    \node[state,scale=0.67] (q0) {$s$};
    \node[state,scale=0.67] [right of=q0] (q1) {$T$};
    \node[state,scale=0.67] [right of=q1] (q2) {$A$};
    \node[state,scale=0.67] [right of=q2] (q3) {$A$};
    \node[state,scale=0.67] [right of=q3] (q4) {$A$};
    \node[state,scale=0.67] [right of=q4] (q5) {$A$};
    \node[state,scale=0.67] [right of=q5] (q6) {$A$};
    \node[state,scale=0.67] [right of=q6] (q7) {$A$};
    \node[state,scale=0.67] [right of=q7] (q8) {$G$};
    \node[state,scale=0.67] [right of=q8] (q9) {$A$};
    \node[state,scale=0.67] [right of=q9] (q10) {$A$};
    \node[state,scale=0.67] [right of=q10] (q11) {$A$};
    \node[state,scale=0.67] [right of=q11] (q12) {$A$};
    \node[state,scale=0.67] [right of=q12] (q13) {$T$};
    \node[state,scale=0.67] [right of=q13] (q14) {$e$};
    \node[state,scale=0.67,draw=none] [above of=q1] (q15) {$T$};
    \node[state,scale=0.67,draw=none] [right of=q15] (q16) {$A$};
    \node[state,scale=0.67,draw=none] [right of=q16] (q17) {$A$};
    \node[state,scale=0.67,draw=none] [right of=q17] (q18) {$A$};
    \node[state,scale=0.67,draw=none] [right of=q18] (q19) {$A$};
    \node[state,scale=0.67,draw=none] [right of=q19] (q20) {$A$};
    \node[state,scale=0.67,draw=none] [right of=q20] (q21) {$T$};
    \node[state,scale=0.67,draw=none] [right of=q21] (q22) {$A$};
    \node[state,scale=0.67,draw=none] [right of=q22] (q23) {$A$};
    \node[state,scale=0.67,draw=none] [right of=q23] (q24) {$A$};
    \node[state,scale=0.67,draw=none] [right of=q24] (q25) {$A$};
    \node[state,scale=0.67,draw=none] [right of=q25] (q26) {$A$};
    \node[state,scale=0.67,draw=none] [right of=q26] (q27) {$T$};

    \path (q0) edge node {} (q1)
    (q1) edge node {} (q2)
    (q2) edge node {} (q3)
    (q3) edge node {} (q4)
    (q4) edge node {} (q5)
    (q5) edge node {} (q6)
    (q6) edge node {} (q7)
    (q7) edge node {} (q8)
    (q8) edge node {} (q9)
    (q9) edge node {} (q10)
    (q10) edge node {} (q11)
    (q11) edge node {} (q12)
    (q12) edge node {} (q13)
    (q13) edge node {} (q14);
  \end{tikzpicture}
  \end{center}
  \end{mdframed}
  \caption{The same sequence aligned against a different reference graph, showing two apparent SNPs}
  \label{fig:heuristic_encouragement_b}
  \end{subfigure}
\end{figure}
\end{document}