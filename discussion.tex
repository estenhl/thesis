\documentclass[thesis.tex]{subfiles}

\begin{document}
\chapter{Discussion}
\label{sec:discussion}
We will in this chapter discuss the results which were presented in the preceding chapters. The chapter will highlight interesting outcomes of the tests and discuss the impact they have on the viability of the approach. The discussions will form the basis for the two subsequent chapters.
\section{Is the approach correct?}
We start by discussing the results from Chapter \ref{sec:validation}. Every test case in the chapter comes with a set of formal statements regarding the provided visualization, which represents the underlying result. We are not interested in arguing whether these statements are true or not: We want to discuss if the statements provide a valid basis for confirming the correctness of the approach. \\
\par\noindent
In the introduction of the chapter, we describe omitting trivial statements to avoid tedious lists of uninteresting properties. By classifying a distinct set of traits as trivial, we implicitly classify another, disjoint set as non-trivial. We have not chosen statements from this set exhaustively: We have chosen a set of statements which we consider to be non-trivial, but still general enough to describe the properties of the most fundamental, underlying, mathematical version of the alignment problem. When we envision the approach being utilized for a specific biological problem, we would assume the need for a more specific set of statements. These could stem from domain knowledge from the exact biological question being answered, such as 
\begin{displayquote}
"Every valid alignment should only contain vertices from a separable subset of the population represented in the graph."
\end{displayquote}
Or they could originate from a statistical analysis point of view, for instance
\begin{displayquote}
"The fraction of input sequences traversing a path should impact the alignment score of every alignment against that path."
\end{displayquote}
We claim these statements are unambiguously extensions to the set of statements provided in this thesis, and thus they strictly represent further specifications of the problem being solved. Although they might be necessary adjustments to turn the approach into an applicable solution to real life problems, we argue the algorithm as presented here has an innate value in its universality, as a proof of concept: A basis easily modifiable for more specific scenarios. We conclude that the approach is correct for the problem it is meant to solve, and although this problem is a simplified version it is situated at the core of more specialized applications.\\
\par\noindent
The structural variation as it is presented in the chapter displays a weakness of the approach. In section \ref{sec:impl_heuristical} we mention the concept of a split alignment when discussing the heuristics, which can be a result of such variation. The approach is able to locate the data which is needed to identify a split alignment, but it is not able to handle it. We imagine this could be done through a more specified scoring schema, for instance one which allows a large gap. This represents an increase in conceptual complexity and is left for others.
\section{Is the approach efficient?}
We will move on to discuss the results found in Chapter \ref{sec:performance}. Specifically, there are three complexity related characteristics with these results we find interesting: The efficiency of the approach under optimal conditions, the complexity in relation to introducing fuzzyness and the indexation. The three subsequent sections will summarize these characteristics, discuss the nature of their results and the effects this has on the viability of the approach. The results from the accuracy tests will be discussed separately in section \ref{sec:heuristical_applications} and also to a large degree form the fundament for possible future work discussed in Chapter \ref{sec:future_work}.
\subsection*{Optimal conditions}
We define the base case in the experiments done as the problem of aligning an unaltered read back to its origin. We test this by putting constraints on the parameters concerned with fuzzyness and vary the remaining variables, typically related to the complexity of the input data. The results can be seen in figures \ref{fig:runtime_G_log}, \ref{fig:runtime_G_small}, \ref{fig:runtime_s} and \ref{fig:runtime_b}. The first set of tests were concerned with determining the relationship between execution time and the number of vertices in the graph. As expected PO-MSA shows a clear linear relation over the entire sample it was able to run. The fuzzy search is only dependant on the graph size to determine the depth of the suffix tree which is searched, and this logarithmical relationship looks almost constant in comparison. In the tests examining different sequence lengths, we also see a much smaller growth in our approach then what can be seen in PO-MSA, although both appear to be linear. The reason for the different linear factors can be caused by the number of vertices which are traversed by the exhaustive search. Every such vertex has to do a number of operations correlated to the sequence length, by reducing this number we impact the total number of operations necessary. The tests concerned with branching factor shows no distinct indications on which approach should be preferred.\\
\par\noindent
The results from these three tests lead us to conclude that the fuzzy search algorithm outperforms PO-MSA by a large factor in what we have defined as the base cases: Aligning an unaltered read back to its origin. This in itself is not an extremely impressive result: It can be achieved through simpler solutions, for instance by using hashed k-mers of length $|r|$ from the graph as an index. What is interesting is that this is not the characteristic which was identified as the goal of the approach and then sought out in a vacuum: It emerges from a solution to a more general problem. This shows the approach is very tractable for the simplest use cases.
\subsection*{The exponential growth in relation to fuzzyness}
We increase the complexity of the test cases by introducing fuzzyness through the error margin parameter $\lambda$. The results can be seen in figure \ref{fig:runtime_lambda}, where a large exponential growth is displayed. This is in a way expected: Fuzzyness has the innate property of exponentially increasing the number of interesting possibilities. When we combine this with a dense search space of similar sequences, the growth rapidly increases. At some point, it is better to exhaustively let PO-MSA test the possibilities we actually have instead of letting the fuzzy search generate and search for variants. The border between the two is represented where the two functions cross in the figure.\\
\par\noindent
The extreme growth and the early cross-over point between the two algorithms in the figure indicate that our approach has limited applications when faced with large amounts of variation. In figure \ref{fig:runtime_lambda_parallell} there are results which can refute this conclusion to some degree. We can see that a simple parallelization seems to put a linear bound on the exponential growth. There still exists room for large optimizations through parallelizing the suffix tree search\footnote{As presented in section \ref{sec:impl_parallelization}}; we included a simple version to display the powerful effect. However, the most important point is seen in relation to graph size: Although the function grows fast, the starting point is decided by the ``base case'' of no fuzzyness, a behaviour we know will improve as the size of the input data increases. Figure \ref{fig:projected_lambda} shows the results from the larger data set projected onto the plot displaying the results from the smaller data set\footnote{The results seen in figures \ref{fig:runtime_lambda} and \ref{fig:runtime_lambda_parallell}}. The fact that these two factors do not seem to interact with each other results in an increase in tractability as the data sets grow: A very desirable trait when dealing with real genetic data. \\
\par\noindent
Another important argument can be made as the graph size increases in comparison to the read length. There will exist a larger number of pure combinations of paths of length $|r|$ in the graph, which decreases the probability of not finding anything relevant. This, in turn, leads to a decreased need for a high $\lambda$ value. This information leave us arguing that the approach still presents interesting opportunities when dealing with real data.
\begin{figure}
  \begin{tikzpicture}[trim axis left, trim axis right]
    \begin{axis}[scale only axis,height=0.5\textwidth,width=0.5\textwidth,xmin=0,ymin=0,xmax=5,ymax=35000,scaled ticks=false, legend pos=north west, xlabel={$\lambda$}, ylabel={Milliseconds},xtick={0,1,2,3,4,5}, legend cell align=left, ylabel near ticks]
      \addplot[color=blue,mark=*] coordinates {
        (0, 29)
        (1, 152)
        (2, 636)
        (3, 1700)
        (4, 8856)
        (5, 27642)
      };
      \addplot[color=green,mark=*] coordinates {
        (0, 39)
        (1, 207)
        (2, 1109)
        (3, 4438)
        (4, 15048)
        (5, 31101)
      };
\addlegendentry{$|G|=35.770$}
\addlegendentry{$|G|=144.480$}
\end{axis}
\end{tikzpicture}
\caption[Runtime as a function of $\lambda$ over different graph sizes]{Runtime as a function of $\lambda$ over different graph sizes}
\label{fig:projected_lambda}
\end{figure}
\subsection*{Indexation}
\label{sec:discussion_indexation}
One of the important results presented in the previous chapter is the amount of time used by the indexation process, as seen in figures \ref{fig:index_constituents_explicit} and \ref{fig:comparison_align}. Most of this time is used by slow interactions with the file system, i.e. by reading and writing a large index. This thesis has not at all been concerned with the tractability of this process. Although the results seem severe there exists solutions both for better compression \cite{a_block_sorting_lossless_data_compression_algorithm}\cite{compression}, better serialization \cite{jvm_serialization} and smarter interactions with slower hardware \cite[Section 4.7]{data_structures_and_algorithm_analysis_in_java}. Ultimately one could argue that in a real life scenario the index should be kept in memory on a supercomputer \cite{multiple_sequence_alignment_on_supercomputers}\cite{supercomputer}. Importantly, we are not concluding that it is fair to completely remove the indexation factor when examining the results. We are rather stating this is a key component of the developed approach, but the work is left for others.
\section{A comparison between the sequence graphs tool and the "fuzzy context-based alignment" tool}
\label{sec:conceptual_comparison}
In the previous chapter, we presented some results from a set of tests run as a comparison between the sg tool and our tool. We made some arguments as to why this comparison was hard to do, and what problems we faced. Because of these, we will in this section focus on the conceptual differences in the two algorithms and the impact the divergence between them have on the results, both in light of the alignments they produce and what differences in time complexity can be expected. We will start by describing the approaches step by step to have a more fine-grained picture for the comparison. Throughout this section, we will refer to the approach developed by Novak et al. as sg and our approach as fuzzy search.\\
\par\noindent
Both algorithms start out by searching for contexts. Already here there is a separation through the requirement for uniqueness, but we see this as a clear design choice taken based on domain knowledge. When we also take into account the previously described triviality of changing between the two choices we see this as a difference which is not that interesting in this setting. When the contexts have been found, the algorithms start to diverge. The fuzzy search picks out the vertice in ``center'' of the context and stores it in a candidate set. We later search through the candidate sets with an exhaustive search algorithm. Sg locks the entire context on to the string and seeks to increase the locked portion by combining overlapping contexts. The overlapped contexts are further expanded by combining them with allowed gaps in between, and finally doing a bounded search to fill in the remaining gaps.\\
\par\noindent
We can go further in comparing the two by describing the sg algorithm in terms of the data structures used when presenting our approach. Because of the uniqueness restriction, each candidate set would either contain a single vertex or be empty. Both singular contexts and the combination of these would be represented by consecutive candidate sets containing consecutive vertices. The second search done to combine non-overlapping contexts would be similar to the search we do, a bounded search seeking to find ``missing'' candidate vertices for a short consecutive number of candidate sets. The final search is required when we have consecutive empty candidate sets spanning a number of indexes larger than a given mismatch parameter. This search could, for instance, be implemented as a PO-MSA search starting in the vertex in the last non-empty candidate set preceding the gap and ending in the vertex in the first non-empty candidate set succeeding the gap.\\
\par\noindent
When we have both algorithms on this level of detail, the separation between them becomes apparent. Our approach is based on the assumption that \textit{if we localize all interesting areas, an optimal solution is bound to be in the combination of them}. The sg approach seems based on an assumption along the lines of \textit{if we find enough uniquely identifiable areas, we can combine these into a good solution}. We can see a clear cut distinction between a non-heuristical and a heuristical approach, which will have an effect on the quality of the results. There are two points which separate the algorithms in regards to runtime: Firstly, we search for a complete number of candidates, a number which grows exponentially related to the allowed fuzzyness. We also do an exhaustive search on all the possible recombinations, another exponential factor. The results from this separation should become apparent in both algorithms when faced with non-optimal cases, represented by searching for strings which do not have a good counterpart in the graph. Sg would see a decrease in quality of the results while our approach sees a growth in runtime. Both can argue these are cases they are not meant specifically to handle. 
\section{Heuristical applications}
\label{sec:heuristical_applications}
The largest disadvantage of the approach which became apparent through the performance testing was the exponential growth in relation to introducing fuzzyness. Figure \ref{fig:correctness_both} depicts the accuracy of the algorithm as we modify the error margin parameter for datasets with varying amounts of noise. In figure \ref{fig:correctness_both_heur} we can see the same set of tests run on the heuristical algorithm. The figures indicate that this modification can achieve better results with a lower error margin, a trait which will increase the tractability of the approach by limiting the factor which has the greatest impact on the time complexity. We will start this section by formally arguing that this observation is correct.\\
\par\noindent
The heuristical functionality in the modified version of the algorithm will only influence the execution whenever optimality can not be guaranteed. This means the heuristical version will always find every optimal alignment which the non-heuristical algorithm can find. Because the two algorithms are equivalent in this regard, one might think the burden of the heuristicality would lie in introducing ambiguity. In our controlled test environment we have decided accuracy through comparing the results with a benchmark; A setting which is less viable in real life applications. The algorithm does, however, have built in a self-validating procedure: Whenever we find an alignment with a score $\varphi_A \geq T$ we can be certain it is optimal whether the algorithm is run heuristically or not.\\
\par\noindent
In cases where the score drops below $T$ we are faced with ambiguity we can not get around. We can still make assumptions on the quality of the alignment based on the input data: The heuristical algorithm will work better with input sequences which have a uniform distribution of noise. When the variation is spread out across multiple contexts, it will be able to identify candidate vertices with a lower $\lambda$. The number of correctly recognized candidate vertices which make it through the pruning has a direct impact on the final result. We can also assess the quality based on the difference in $\varphi_A$ and $T$. These metrics provide us with tools for deciding when the heuristical result is good enough, and when we need to increase the error margin. The fact that the heuristical algorithm is always correct when the non-heuristical is and we have a method for validating it, shows it is a powerful addition to the regular approach.\\
\par\noindent
We finished the last section by classifying the sequence graphs algorithm as heuristical and our approach as non-heuristical. We discussed the implications this has and argue they both provide valuable elements in the search for an optimal solution to the problem of alignment. If we let these two represent the extremes in a space we argue our heuristical modification is somewhere in between them. It is less complex than the normal fuzzy search algorithm due to the decreased requirement for a high error margin, but still more exhaustive than sg. It is important to once again point out that the progress towards sg in terms of reducing complexity does not come at the cost of a reduction in accuracy. This is an enticing trait for a heuristical modification.\\
\par\noindent
We will present one more possibility for our approach in combination with other heuristical methods: A tool for validation. Running an alignment on a different heuristical tool produces a score, which can be used to calculate an error margin. Running our tool with the error margin results in one of three cases:
\begin{itemize}
  \item An alignment with the same score, meaning one of these exist
  \item No alignment, meaning the score is faulty and the alignment is wrong
  \item An alignment with a higher score, meaning the heuristical method was wrong
\end{itemize}
The first case provides an obvious value if it is combined with a procedure for checking whether a given alignment has a given score. The second case will discover a bug in the heuristical procedure, which is definitively valuable, but not on a conceptual level. The third case is very interesting as it does not necessarily reveal a fault in the heuristical implementation, but more so a weakness in the heuristic used. When utilizing our approach as a validator, one can set an upper bound to the values of $\lambda$ which should be validated to stay in the feasible region. This allows us to create a set of confirmed optimal, high scoring reads and a set of heuristically aligned, low scoring reads. This is information which is interesting for instance if the result is a set of variants.
\chapter{Conclusion}
The aim of this thesis was to present the approach we developed, and through testing its validity and efficiency determine whether it represents a feasible solution to the problem of aligning text strings against graphs. We have shown that the implementation provides expected results and argued for the validity of these results as a solution to the most general form of the problem. We have also presented modifications we see as necessary for this approach to be translated towards more specific biological problems. The performance results were more ambiguous, revealing that the algorithm has both strengths and weaknesses. We have discussed both of these in the preceding chapter, while also proposing possible solutions to some of the arising problems. The fact that there  are still shortcomings leads us to conclude that our approach can not serve as an optimal solution to the alignment problem. However, because of the feasibility shown by the heuristically modified algorithm, we do conclude it is an important step along the way towards this goal. We will briefly present our thoughts on where this path could lead in the subsequent chapter.
\end{document}
