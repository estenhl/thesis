\documentclass[thesis.tex]{subfiles}

\begin{document}
\chapter{Discussion}
\section{Is the approach correct?}
We will start by discussing the results from Chapter 5. Every test from the chapter comes with a set of formal statements regarding the provided visualization, which represents the underlying result. We are not interested in arguing whether these statements are true or not: We want to explore if the statements provide a valid basis for confirming the correctness of the approach. In the introduction of the chapter we state that non-trivial statements are dropped, to avoid long lists of general very general assumptions. Doing this cut off also has a secondary, interesting property: It demonstrates which properties we find non-trivial, and thus interesting to examine. In order to use the approach for actual biological problems one would assume the need for more of these non-trivial statements. They could originate from a problem specification in a precise biological domain: \textit{``All alignments should contain vertices from an separable subset of the population''}, or for instance from a statistical analysis point of view: \textit{``The fraction of input sequences traversing a path should impact all scores retrieved through aligning against it''}. We argue these are strictly examples of extensions to the statements we present, and therefore also represent extensions to the requirement specification used as a basis for the approach. We see these as necessary extensions, and they will be discussed further in chapter \ref{sec:future_work}. But we also claim the work done in this project has an innate value, as a search for a solution to a more general problem. All knowledge gained in reference to this general problem is easily transferable to more specific use cases.
\section{The performance}
We will move on to discuss the results found in Chapter 6. Specifically there are three characteristics with these results we find interesting: The efficiency of the approach under optimal conditions, the complexity in relation to introducing fuzzyness and the indexation. The three subsequent chapters will summarize these characteristics, discuss the nature of the results and the effects this has on the viability of the approach.
\subsection*{Optimal conditions}
We define the base case in the experiments done as the problem of aligning an unaltered read back to its origin. We test this by putting constraints on the parameters concerned with fuzzyness and vary the remaining, typically related to the complexity of the input data. The results can be seen in figure \ref{runtime_G}, \ref{runtime_s} and \ref{runtime_b}. In all of these situations the approach display good behaviour when compared to PO-MSA, especially when the runtime is seen as a function of graph size. This in itself is not an extremely impressive result: It can be achieved through simpler solutions, for instance by using hashed k-mers from the graph as an index. What is interesting is that this is not the characteristic which was identified as the goal of the approach and then sought out in a vacuum. It emerges from a solution to a more general problem. This creates a good foundation by showing that the approach is very tractable for the simplest use cases.
\subsection*{The exponential growth in relation to fuzzyness}
We increase the complexity of the test cases by introducing fuzzyness through the error margin parameter $\lambda$. The results can be seen in figure \ref{fig:runtime_lambda}, where the large exponential growth of the approach can be seen. This is in a way expected: Fuzzyness has the innate property of exponentially increasing the number of interesting probabilities. When we combine this with a dense search space of similar sequences the growth explodes. At some point it is better to exhaustively let PO-MSA test the possibilities we actually have instead of letting the fuzzy search generate and search for variants. The border between the two is represented where the two functions cross in the figure.\\
\par\noindent
The extreme growth and the early cross over point between the two algorithms in the figure seem to make the feasibility of the approach plummet. In the figure \ref{fig:runtime_lambda_parallell} there are results which can refute this statement to some degree. We can see that a simple parallellization seem to flatten the growth. There still exists room for extreme optimizations through parallellizing the suffix tree search; we included a simple version to display the powerful effect. These possibilities are discussed further in chapter \ref{sec:future_work}. The most important point is the one which was made while presenting the results: Although the growth is bad, the starting point is decided by the ``base case'', which we know will improve as the complexity of the input data increases. The fact that these two factors does not interact with each other results in an increase in tractability as the data sets grow: A very desirable trait when dealing with real genetic data. Another important argument can be made as the graph size increases in comparison to the read length. There will exists a large number of pure combinations of paths of length $|r|$ in the graph, which decreases the probability of not finding anything relevant when doing alignments and similarily decreases the necessity for a high error margin. These two pieces of information leave us arguing that the approach is still an interesting step along the way towards solving the problem.\\
\par\noindent
Lastly we want to discuss the nature of the error margin. When working on ``real'' data with an unknown number of errors, the fraction of correctly aligned reads increases as the error margin-parameter is increased (Figure \ref{correctness_both}). The errors in this figure is sequences where the algorithm returns an empty alignment, stating that there exists no way of possibly aligning the sequence within the given error margin. If we chose to view the alignment as a continuous set of discrete contexts, this could happen even when the algorithm has been able to produce a ``good enough'' alignment for every distinct context and combined them in the best possible way. In this process the algorithm could dismiss a lot of actually optimal alignments. This is the process which we allow through the \texttt{--heuristic=true} option which is discussed earlier. Utilizing this decreases the necessary error margin from \textit{number of total errors} $e_t$ to $e_t*(|c|/|s|)$. This is a clear optimization towards making the approach more feasible, but as it moves into the realm of the heuristic it has been avoided throughout this thesis.
\subsection*{Indexation}
The possibly most visually outstanding result presented in the previous chapter is the pure amount of time used by the indexation process, as seen in figures \ref{fig:index_constituents_explicit} and \ref{fig:comparison_align}. Most of this time is used by slow interactions with the file system, by reading and writing a large index. This thesis has not at all been concerned with the tractability of this process. Although the results seem severe there exists solutions both for better compression\textcolor{red}{[ref]}, better serialization\textcolor{red}{[ref]} and smarter interactions with slower hardware\textcolor{red}{[ref]}. Ultimately one could argue that in a real life scenario the index should be kept in memory on a supercomputer\textcolor{red}{[ref]}\textcolor{red}{[ref]}. Importantly, we are not concluding that it is fair to completely remove the indexation factor when examining the results. We are rather stating this is a key component of the developed approach, but the work is left for others (see Chapter \ref{sec:future_work}).
\section{A comparison between the sequence graphs tool and the "fuzzy context-based alignment" tool}

\section{Usefulness of the approach}
The discussion around the results from Chapter 6 show some blatant weaknesses with the approach. However, it also shows that in some cases the approach works great. We will reserve this section to discuss directly what the usefulness of the approached we presented in this thesis. The possibly more interesting discussion regarding how we can utilize what we have learned to improve the algorithm have been reserved for Chapter \ref{future_work}, which also includes a discussion around heuristical applcations.\\
\par\noindent
The results show a clearly separable set of use cases where the approach works great: Aligning small, similar sequences against a large graph. However, the notion of similarity can be hard to decide in advance. For this reason we present a specific realm of usage for the approach: Validation of heuristical methods. If one aligns a sequence against a graph with a heuristical approach one would find an alignment, and a correspondant alignment score. Using this alignment score one could easily calculate an error margin. Running our approach with this error margin would result in one of three cases:
\begin{itemize}
  \item An alignment with the same score, meaning one of these exist
  \item No alignment, meaning the score is faulty and the alignment is wrong
  \item An alignment with a lower score, meaning the heuristical search was incorrect
\end{itemize}
The first case provides an obvious value if it is combined with a procedure for checking whether a given alignment has a given score. The second case will discover a bug in the heuristical procedure, which is definitively valuable but not on a conceptual level. The third case is extremely interesting as it does not necessarily point out a fault in the heuristical implementation, but moreso a conceptual error in the heuristic used.
\end{document}
