\documentclass[thesis.tex]{subfiles}

\begin{document}
\chapter{Discussion}
\section{Is the approach correct?}
We will start out by discussing the results from Chapter 6, specifilcally whether the approach are able to produce the intuitively pleasing feeling mentioned in the beginning of the Chapter. For the creators this is an easy question: Yes, because we designed it to do so. A more interesting formulation of the question could be\\
\par\noindent
\centerline{\textit{"Is the intuition behind the approach correct?"}}\\
\par\noindent
This might be a harder question to answer, probably particularily for the people who inhibits this intuition. \\
\par\noindent
The results from the approach exhibit a set of traits, determined by the underlying definitions and design choices. One could imagine the presence of other traits: Maybe the only paths valid for alignment contain vertices exclusively from a single individual, a set of individuals displaying a common phenotype or a demographically bounded subset of the population. Or maybe one would want to score an alignment not purely based on the alignment score but also include the probability of traversing exactly that given path. We would argue these are variants of strictly extensions to the properties seen in the results from our approach. We have been concerned with developing a solution to a more fundamental problem which can hopefully be used either as a starting point or at least as insipiration when solving more specific biological problems. 
\section{Discussing the performance}
We will move on to discuss the results from Chapter 7. Specifically there are three characteristics with these results we find interesting: The efficiency of the approach under optimal condititions, the complexity in relation to introducing fuzzyness and the indexation process. Shortly these can be described as good, bad and bad: The three subsequent sections will discuss each of them in detail.
\subsection*{Optimal conditions}
We let optimal conditions describe the operation of aligning unaltered reads back to their origin. The results from this can for example be seen in figure \ref{results_g} and they look very good. This is in itself not an exceptionally interesting result. We could for instance solve this even more efficiently by indexing k-mers with the same length as the reads in a hashmap. What is interesting is that this characteristic is not what is sought out in a vacuum: It emerges from a solution to a more general problem. This lays a very good foundation for how the algorithm should behave in more specific situations. 
\subsection*{The exponential growth in relation to fuzzyness}
The extreme exponential growth in complexity in relation to the amount of allowed fuzzyness is a major setback for the approach in general. This is in a way expected, as this is by far the most conceptually challenging part of the problem. Introducing fuzzyness has the innate property of exponentially increasing the number of valid results. When this is combined with a densely populated problem space containing extremely similar text-strings the result is an explosion of possibilities. At some point it is better to simply find the best solution in the actual data one possesses rather than to trying to imagine every one of these possibilities and searching for that. We find the border between the two in the exact point where the PO-MSA algorithm and the fuzzy algorithm cross paths in figure \ref{fig:runtime_lambda_large}.\\
\par\noindent
Althought this growth seem to make the feasibility of the approach plummet, there are some light at the end of the tunnel. Firstly the main part of the complexity stems from the pure number of recursive calls done in the suffix tree search, as a result of not being able to prune. As previously mentioned this is stands out as an extremely good candidate for parallellization. Most importantly, the exponential growth starts from a starting point which is shown to be very promising in relation to both the graph size and the graph complexity. This means the tractability increases as the size of the datasets grow, a very desirable trait when dealing with real genetic data. Another important point can be made as the graph size $|G|$ increases in comparison to the read length $|r|$: There will exist a larger number of paths of length $r$ thus reducing the need for a high error margin. These two pieces of information leaves us arguing that the approach is still a valid step along the way towards solving the problem.
\subsection*{Slow indexation: A death blow to the approach?}
A short answer to this question is no. Most of the time spent by the indexation is used doing slow interactions with the file system, reading and writing a large index. There exists solutions both for better compression\textcolor{red}{[ref]}, better serialization\textcolor{red}{[ref]} and smarter interactions with slower hardware\textcolor{red}{[ref]}. If all else fails one could argue the index should be kept in memory on a supercomputer somewhere\cite{multiple_sequence_alignment_on_supercomputers}.\\
\par\noindent
Importantly, we are not drawing the conclusion that the indexation factor can be completely removed when examining the results. If the approach simply reduces time complexity by naively increasing the space complexity this is not in any way an impressive result. We are simply stating that the indexation process has not been a priority throughout this project. The approach relies on a more sophisticated index to increase its tractability, but this work is left for others.
\section{A comparison between the sequence graphs tool and the "fuzzy context-based alignment" tool}
The results produced by the two tools are amb
Both algorithms starts out by finding contexts. Already at this point there is a separation as Novak et. al has a requirement for uniqueness whereas the fuzzy algorithm does not. In theory this will only have an impact on the size of the suffix tree which is searched. This is a clear design choice which is taken dependant on which problem one wants to solve and is thus not that interesting. The trivialities of jumping between the two choices are described in the Implementation section. Once the contexts have been found the real differences start to become apparent. The sg algorithm stores the contexts in full length  while the fuzzy algorithm picks out the center vertice of each one and stores this. The former tries to find even longer matching contexts by finding overlap between the ones that have already been found, filling in the gaps through regular graph searches. The latter searches for an optimal path through the vertices that were found in the preceding step.\\
\par\noindent
We can go one technical step further in the comparison by explaining the sg algorithm through the data structures which were used to explain the fuzzy algorithm. The candidate sets would either have a single vertex or be empty, following from the requirement of uniqueness.  A consecutive set of candidate sets containing a vertex would correspond to either a single context, or a set of overlapping contexts. Finding an optimal path through these are easy. Filling in the gaps would be equivalent to finding vertices for the empty candidate sets. This would be done for instance by a full PO-MSA search starting in the vertex in the last non-empty candidate set preceding the ``hole'' and ending in the vertex of the first non-empty candidate set succeding it.\\
\par\noindent
If we now take a step back and look at the two approaches on a more abstract level we can examine what impact these small details have on the results produced. The first procedure can be conceptually summarized as \textit{if we identify enough areas which are uniquely similar between the graph and the sequence, we can easily fill in the gaps between them and achieve a good result}. The fuzzy algorithm can similarily be contracted into the statement \textit{if we identify all areas which are similar between the graph and the sequence the combination of them will contain an optimal result, which we can find}. Describing the two on this level makes an intuitive comparison easy: They have a similar foundation in the way they approach the problem, but the latter will guarantee optimality at the cost of computational complexity. This effect would become clear if one does an alignment of a string very different from anything that exists in the graph with a high enough threshold value. Sg would move away from an optimal situation by producing a bad result, while fuzzy would move away from an optimal situation by using a lot of time. Both could defend the bad performance by arguing that is not a case they are meant to handle.
\label{sec:comparison_discussion}
\section{Non-heuristicality and the error-margin}
As early as the first sections of this thesis we refer to the approach as non-heuristical. Throughout most of Chapter 7 we drop displaying the correctness because we know the algorithm always achieve the correct result. However, in the section containing correctness as a function of noise we present figure \ref{fig:correctness_both}: A figure which shows the approach can also be incorrect. Importantly, this is not because the tool gives a wrong result. It is because the tool refuses to give a result at all. The algorithm has programmed in the notion that \textit{Whenever i can't confirm I'm correct, I assume im wrong}. Through this notion the approach puts alot of weight on the user, by assuming he or she is able to set the correct error-margin. Due to the exponential growth in complexity with regards to this parameter, this could it is not feasible to run the algorithm with the correct value. These would be cases where the ``This is not what the approach is meant to do''-argument would come in to play: A too high mismatch between the input sequence and the graph simply means there is no existing correlation. The exponential growth also provides an opportunity for users: When running the algorithm iteratively on increasing error-margin value, the largest chunk of computation will always come from the latest search which allows the largest amount of fuzzyness. This is a process which can be run either until a result is produced or until the parameter is so large the search is not tractable anymore.\\
\par\noindent
If one assumes users do not want to go around guessing values for the involved parameters, another area of usage presents itself: A validation tool. If one runs a heuristical approach and finds an alignment with a given score, one can use this score to determine an error-margin which one runs the tool with and get one of three results:
\begin{itemize}
  \item The score found is optimal
  \item The alignment found is not correct, no alignments produce such a high score
  \item The alignment found is not optimal, here is one which has a higher score
\end{itemize}
The first result is somewhat vague, due to the fact that the tool would spit out any alignment with this score and not necessarily confirm whether the previously found alignment is correct. A huge added bonus for the third case in which the approach actually gives a better alignment. The second case is somewhat futile, because no value is added, but atleast it displays a bug in the original heuristical algorithm.
\end{document}
