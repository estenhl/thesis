\documentclass[thesis.tex]{subfiles}

\begin{document}
\chapter{Discussion}
\section{Is the approach correct?}
We will start by discussing the results from Chapter 5. Every test from the chapter comes with a set of formal statements regarding the provided visualization, which represents the underlying result. We are not interested in arguing whether these statements are true or not: We want to explore if the statements provide a valid basis for confirming the correctness of the approach. In the introduction of the chapter we state that non-trivial statements are dropped, to avoid long lists of general very general assumptions. Doing this cut off also has a secondary, interesting property: It demonstrates which properties we find non-trivial, and thus interesting to examine. In order to use the approach for actual biological problems one would assume the need for more of these non-trivial statements. They could originate from a problem specification in a precise biological domain: \textit{``All alignments should contain vertices from an separable subset of the population''}, or for instance from a statistical analysis point of view: \textit{``The fraction of input sequences traversing a path should impact all scores retrieved through aligning against it''}. We argue these are strictly examples of extensions to the statements we present, and therefore also represent extensions to the requirement specification used as a basis for the approach. We see these as necessary extensions, and they will be discussed further in chapter \ref{sec:future_work}. But we also claim the work done in this project has an innate value, as a search for a solution to a more general problem. All knowledge gained in reference to this general problem is easily transferable to more specific use cases.
\section{The performance}
We will move on to discuss the results found in Chapter 6. Specifically there are three characteristics with these results we find interesting: The efficiency of the approach under optimal conditions, the complexity in relation to introducing fuzzyness and the indexation. The three subsequent chapters will summarize these characteristics, discuss the nature of the results and the effects this has on the viability of the approach.
\subsection*{Optimal conditions}
We define the base case in the experiments done as the problem of aligning an unaltered read back to its origin. We test this by putting constraints on the parameters concerned with fuzzyness and vary the remaining, typically related to the complexity of the input data. The results can be seen in figure \ref{runtime_G}, \ref{runtime_s} and \ref{runtime_b}. In all of these situations the approach display good behaviour when compared to PO-MSA, especially when the runtime is seen as a function of graph size. This in itself is not an extremely impressive result: It can be achieved through simpler solutions, for instance by using hashed k-mers from the graph as an index. What is interesting is that this is not the characteristic which was identified as the goal of the approach and then sought out in a vacuum. It emerges from a solution to a more general problem. This creates a good foundation by showing that the approach is very tractable for the simplest use cases.
\subsection*{The exponential growth in relation to fuzzyness}
We increase the complexity of the test cases by introducing fuzzyness through the error margin parameter $\lambda$. The results can be seen in figure \ref{fig:runtime_lambda}, where the large exponential growth of the approach can be seen. This is in a way expected: Fuzzyness has the innate property of exponentially increasing the number of interesting probabilities. When we combine this with a dense search space of similar sequences the growth explodes. At some point it is better to exhaustively let PO-MSA test the possibilities we actually have instead of letting the fuzzy search generate and search for variants. The border between the two is represented where the two functions cross in the figure.\\
\par\noindent
The extreme growth and the early cross over point between the two algorithms in the figure seem to make the feasibility of the approach plummet. In the figure \ref{fig:runtime_lambda_parallell} there are results which can refute this statement to some degree. We can see that a simple parallellization seem to flatten the growth. There still exists room for extreme optimizations through parallellizing the suffix tree search; we included a simple version to display the powerful effect. These possibilities are discussed further in chapter \ref{sec:future_work}. The most important point is the one which was made while presenting the results: Although the growth is bad, the starting point is decided by the ``base case'', which we know will improve as the complexity of the input data increases. The fact that these two factors does not interact with each other results in an increase in tractability as the data sets grow: A very desirable trait when dealing with real genetic data. Another important argument can be made as the graph size increases in comparison to the read length. There will exists a large number of pure combinations of paths of length $|r|$ in the graph, which decreases the probability of not finding anything relevant when doing alignments and similarily decreases the necessity for a high error margin. These two pieces of information leave us arguing that the approach is still an interesting step along the way towards solving the problem.\\
\par\noindent
Lastly we want to discuss the nature of the error margin. When working on ``real'' data with an unknown number of errors, the fraction of correctly aligned reads increases as the error margin-parameter is increased (Figure \ref{correctness_both}). The errors in this figure is sequences where the algorithm returns an empty alignment, stating that there exists no way of possibly aligning the sequence within the given error margin. If we chose to view the alignment as a continuous set of discrete contexts, this could happen even when the algorithm has been able to produce a ``good enough'' alignment for every distinct context and combined them in the best possible way. In this process the algorithm could dismiss a lot of actually optimal alignments. This is the process which we allow through the \texttt{--heuristic=true} option which is discussed earlier. Utilizing this decreases the necessary error margin from \textit{number of total errors} $e_t$ to $e_t*(|c|/|s|)$. This is a clear optimization towards making the approach more feasible, but as it moves into the realm of the heuristic it has been avoided throughout this thesis.
\subsection*{Indexation}
The possibly most visually outstanding result presented in the previous chapter is the pure amount of time used by the indexation process, as seen in figures \ref{fig:index_constituents_explicit} and \ref{fig:comparison_align}. Most of this time is used by slow interactions with the file system, by reading and writing a large index. This thesis has not at all been concerned with the tractability of this process. Although the results seem severe there exists solutions both for better compression\textcolor{red}{[ref]}, better serialization\textcolor{red}{[ref]} and smarter interactions with slower hardware\textcolor{red}{[ref]}. Ultimately one could argue that in a real life scenario the index should be kept in memory on a supercomputer\textcolor{red}{[ref]}\textcolor{red}{[ref]}. Importantly, we are not concluding that it is fair to completely remove the indexation factor when examining the results. We are rather stating this is a key component of the developed approach, but the work is left for others (see Chapter \ref{sec:future_work}).
\section{A comparison between the sequence graphs tool and the "fuzzy context-based alignment" tool}
In the previous chapter we deemed the results from the comparison inconclusive and hard to decipher. Therefore we will in this section focus on the conceptual differences in the two algorithms and the impact the divergence between them have on the results, both in light of the alignments they produce and what differences in time complexity can be expected. We will start by describing the approaches step by step to have a more detailed picture when doing comparisons. Throughout this section we will refer to the approach developed by Novak et al. as sg and our approach as fuzzy search.\\
\par\noindent
Both algorithms start out by searching for contexts. Already here there is a separation through the requirement for uniqueness, but we see this as a clear design choice taken on domain knowledge. When we also take into account the previously described triviality of changing between the two choices we see this as a difference which is not that interesting in this setting. When the contexts have been found the algorithms start to diverge. The fuzzy search picks out the vertice in ``center'' of the context and stores it in a candidate set. We later search through the candidate sets with an exhaustive search algorithm. Sg locks the entire context on to the string and seeks to increase the locked portion by combining overlapping contexts. The overlapped contexts are further expanded by combining them with allowed gaps in between, and finally doing a bounded search to fill in the remaining gaps.\\
\par\noindent
We can go further in comparing the two by describing the sg algorithm in terms of the data structures used in describing our approach. Because of the uniqueness restriction each candidate set would either contain a single vertex or be empty. Both singular contexts and the combination of these would be represented by consecutive candidate sets containing consecutive vertices. The second search done to combine non-overlapping contexts would be similar to the search we do, a bounded search seeking to find ``missing'' candidate vertices for a short consecutive number of candidate sets. The final search is required when we have consecutive empty candidate sets spanning a number of indexes larger than a given mismatch parameter. This search could for instance be implemented as a PO-MSA search starting in the vertex in the last non-empty candidate set preceding the gap and ending in the vertex in the first non-empty candidate set succeding the gap.\\
\par\noindent
When we have both algorithms on this level of detail the separation between them becomes apparent. Our approach is based on the assumption that \textit{if we separate all interesting areas, an optimal solution is bound to be in the combination of them}. Their approach seems based on an assumption along the lines of \textit{if we find enough uniquely identifiable areas, we can combine these into a good solution}. We can see a clear cut distinction between a non-heuristical and an heuristical approach, which will have an effect on the quality of the results. There are two points which separate the algorithms in regards to runtime: Firstly, we search for a complete number of candidates, a number which grows exponentially related to the allowed fuzzyness. We also do an exhaustive search on all the possible recombinations, another exponential factor. The results from this separation should become apparent in both algorithms when faced with non-optimal cases, represented by searching for strings which does not have a good counter-part in the graph. Sg would see a decrease in quality of the results while we see a growth in runtime. Both can argue these are cases they are not meant specifically to handle. The interesting discussion on whether there exists an optimal middle ground is left for Chapter \ref{sec:future_work}.
\section{Usefulness of the approach}
The discussion around the results from Chapter 6 show some blatant weaknesses with the approach. However, it also shows that in some cases the approach works great. We will reserve this section to discuss directly what the usefulness of the approached we presented in this thesis. The possibly more interesting discussion regarding how we can utilize what we have learned to improve the algorithm have been reserved for Chapter \ref{future_work}, which also includes a discussion around heuristical applications.\\
\par\noindent
The results show a clearly separable set of use cases where the approach works great: Aligning small, similar sequences against a large graph. However, the notion of similarity can be hard to decide in advance. For this reason we present a specific realm of usage for the approach: Validation of heuristical methods. If one aligns a sequence against a graph with a heuristical approach one would find an alignment, and a correspondant alignment score. Using this alignment score one could easily calculate an error margin. Running our approach with this error margin would result in one of three cases:
\begin{itemize}
  \item An alignment with the same score, meaning one of these exist
  \item No alignment, meaning the score is faulty and the alignment is wrong
  \item An alignment with a lower score, meaning the heuristical search was incorrect
\end{itemize}
The first case provides an obvious value if it is combined with a procedure for checking whether a given alignment has a given score. The second case will discover a bug in the heuristical procedure, which is definitively valuable but not on a conceptual level. The third case is extremely interesting as it does not necessarily reveal a fault in the heuristical implementation, but more so a conceptual error in the heuristic used. If one chooses to use the approach as a validator, one can set an upper bound to the values of $\lambda$ which should be used to stay in the feasible region of time complexity. Utilizing the approach in this fashion could be interesting for example in assembly, where the result would be a set of confirmed optimal, high scoring reads and a set of heuristically aligned, low scoring reads.
\end{document}
